{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "asZS1eE_xFwl",
        "jyb-2WYjCdo9",
        "FZUCx5HRCi2M",
        "KfWWy_FaCrqv",
        "ZGBnRCvqCt2X",
        "99RFOYv8CxHB",
        "Ipd0lL0DC5RO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter Goals\n",
        "- How to combine multiple models to solve a single problem, usually solved by a single model.\n",
        "\n",
        "# General Lab Guidlines\n",
        "- Visualization.\n",
        "- Modifiable code snippets."
      ],
      "metadata": {
        "id": "vLMrmp4BwwpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Qk35Y1UNwutp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a dataset\n",
        "# dataset names: \"airline\", \"breast-cancer\", \"contact-lenses\", \"cpu\", \"cpu.with.vendor\", \"credit-g\", \"diabetes\", \"glass\", \"hypothyroid\", \"ionosphere\", \"iris.2D\", \"iris\", \"labor\", \"segment-challenge\", \"segment-test\", \"soybean\", \"supermarket\", \"unbalanced\", \"vote\", \"weather.nominal\", \"weather.numeric\"\n",
        "# df = pd.read_csv(\"data/weather.numeric.csv\")\n",
        "# instances = loader.load_file(\"data/weather.numeric.arff\")"
      ],
      "metadata": {
        "id": "73iEUGIr6PvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modules & Datasets Setup"
      ],
      "metadata": {
        "id": "asZS1eE_xFwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!apt-get install default-jdk\n",
        "!apt install libgraphviz-dev"
      ],
      "metadata": {
        "id": "ybaGzIpGq_QT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5da691c-8d7c-4d6b-8d3c-f294fc8bae2f",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "default-jdk is already the newest version (2:1.11-72build2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libgraphviz-dev is already the newest version (2.42.2-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install pygraphviz\n",
        "!pip install python-javabridge\n",
        "!pip install python-weka-wrapper3\n",
        "!pip install sklearn-weka-plugin"
      ],
      "metadata": {
        "id": "ofMkHDFknGUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f56cbdf-8b51-48d2-a71b-f3c1dccfad8a",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygraphviz in /usr/local/lib/python3.10/dist-packages (1.11)\n",
            "Requirement already satisfied: python-javabridge in /usr/local/lib/python3.10/dist-packages (4.0.3)\n",
            "Requirement already satisfied: numpy>=1.20.1 in /usr/local/lib/python3.10/dist-packages (from python-javabridge) (1.23.5)\n",
            "Requirement already satisfied: python-weka-wrapper3 in /usr/local/lib/python3.10/dist-packages (0.2.14)\n",
            "Requirement already satisfied: python-javabridge>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from python-weka-wrapper3) (4.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from python-weka-wrapper3) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from python-weka-wrapper3) (23.2)\n",
            "Requirement already satisfied: configurable-objects in /usr/local/lib/python3.10/dist-packages (from python-weka-wrapper3) (0.0.1)\n",
            "Requirement already satisfied: simple-data-flow in /usr/local/lib/python3.10/dist-packages (from python-weka-wrapper3) (0.0.1)\n",
            "Collecting sklearn-weka-plugin\n",
            "  Using cached sklearn-weka-plugin-0.0.7.tar.gz (69 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sklearn-weka-plugin) (1.23.5)\n",
            "Requirement already satisfied: python-weka-wrapper3>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from sklearn-weka-plugin) (0.2.14)\n",
            "Collecting sklearn (from sklearn-weka-plugin)\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#Restart runtime after installing the dependencies"
      ],
      "metadata": {
        "id": "K7AOVpj6tJCo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import weka.core.jvm as jvm\n",
        "from weka.core import converters\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "z3KWEJT_COnI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "data_dir = 'data'"
      ],
      "metadata": {
        "id": "g-Irnr7lwXOW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#!rm -r weka\n",
        "#!rm -r data"
      ],
      "metadata": {
        "id": "ZDA_uh_swSNP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#jvm.stop()\n",
        "jvm.start(packages=True)"
      ],
      "metadata": {
        "id": "9gqo2Sdkrp1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba13561-c7d2-4f77-c144-a13c0327d268",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:weka.core.jvm:Adding bundled jars\n",
            "DEBUG:weka.core.jvm:Classpath=['/usr/local/lib/python3.10/dist-packages/javabridge/jars/rhino-1.7R4.jar', '/usr/local/lib/python3.10/dist-packages/javabridge/jars/runnablequeue.jar', '/usr/local/lib/python3.10/dist-packages/javabridge/jars/cpython.jar', '/usr/local/lib/python3.10/dist-packages/weka/lib/core.jar', '/usr/local/lib/python3.10/dist-packages/weka/lib/python-weka-wrapper.jar', '/usr/local/lib/python3.10/dist-packages/weka/lib/mtj.jar', '/usr/local/lib/python3.10/dist-packages/weka/lib/weka.jar', '/usr/local/lib/python3.10/dist-packages/weka/lib/arpack_combined.jar']\n",
            "DEBUG:weka.core.jvm:MaxHeapSize=default\n",
            "DEBUG:weka.core.jvm:Package support enabled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Preparing Datasets\n",
        "if not os.path.exists(data_dir):\n",
        "    !mkdir $data_dir\n",
        "    for file in ['airline.arff', 'breast-cancer.arff', 'contact-lenses.arff', 'cpu.arff', 'cpu.with.vendor.arff', 'credit-g.arff', 'diabetes.arff', 'glass.arff', 'hypothyroid.arff', 'ionosphere.arff', 'iris.2D.arff', 'iris.arff', 'labor.arff', 'segment-challenge.arff', 'segment-test.arff', 'soybean.arff', 'supermarket.arff', 'unbalanced.arff', 'vote.arff', 'weather.nominal.arff', 'weather.numeric.arff',]:\n",
        "        url = 'https://git.cms.waikato.ac.nz/weka/weka/-/raw/main/trunk/wekadocs/data/' + file\n",
        "        !wget -P $data_dir $url\n",
        "    loader = converters.Loader(classname=\"weka.core.converters.ArffLoader\")\n",
        "    saver = converters.Saver(classname=\"weka.core.converters.CSVSaver\")\n",
        "    for file in glob.glob(os.path.join(data_dir, '*.arff')):\n",
        "        dataset = loader.load_file(file)\n",
        "        filename, file_extension = os.path.splitext(file)\n",
        "        saver.save_file(dataset, filename + '.csv')\n",
        "    !wget -P $data_dir https://raw.githubusercontent.com/Rytuo/ITMO-CT/master/Others/AdvancedML/data/OpenML/data/1438.arff\n",
        "    !rm -r weka"
      ],
      "metadata": {
        "id": "KOJC_Fuhqvw7",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import weka.core.packages as packages\n",
        "packages.install_package(\"simpleEducationalLearningSchemes\")\n",
        "packages.install_package(\"generalizedSequentialPatterns\")\n",
        "packages.install_package(\"classAssociationRules\")\n",
        "packages.install_package(\"NNge\")\n",
        "packages.install_package(\"LibSVM\")\n",
        "\n",
        "from weka.core.converters import Loader\n",
        "loader = Loader(classname=\"weka.core.converters.ArffLoader\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "25QIdRrXuB5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.1 Combining Multiple Models"
      ],
      "metadata": {
        "id": "LiE_vsjGMlXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.2 Bagging"
      ],
      "metadata": {
        "id": "jyb-2WYjCdo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagging by voting\n",
        "# for each iteration\n",
        "# sample data\n",
        "# train a model\n",
        "# upon classification, call all trained models\n",
        "# classify by voting\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a dummy dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the base classifier\n",
        "base_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Alternatively, you can use a VotingClassifier for comparison\n",
        "voting_classifier = VotingClassifier(estimators=[('base_classifier', base_classifier)], voting='soft')\n",
        "voting_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the VotingClassifier\n",
        "y_pred_voting = voting_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy_voting = accuracy_score(y_test, y_pred_voting)\n",
        "print(f\"Accuracy with VotingClassifier: {accuracy_voting:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQZVsueWIT3k",
        "outputId": "9121041c-b9ed-4c4a-d78d-e2b796770a6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with VotingClassifier: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagging by numeric weighted average\n",
        "# for each iteration\n",
        "# sample data\n",
        "# train a model\n",
        "# upon regression, call all trained models, and take weighted average\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Create a dummy regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the base regressor\n",
        "base_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Define the BaggingRegressor with weighted average\n",
        "bagging_regressor = BaggingRegressor(base_regressor, n_estimators=10, random_state=42)\n",
        "\n",
        "# Fit the BaggingRegressor on the training data\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20WX9HeOM8r8",
        "outputId": "33a91dc1-73fc-4553-acec-90b78051617d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 7484.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12.2.1 Task** Inject noise in some given data set and train a model only on it. Now apply the bagging method and observe how it guards against the noise."
      ],
      "metadata": {
        "id": "0x4DS5DQYn3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.3 Randomization"
      ],
      "metadata": {
        "id": "FZUCx5HRCi2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random subspace method\n",
        "# For each iteration\n",
        "# take a random subset of features, and subset of rows\n",
        "# train a model\n",
        "# upon classification, aggregate all models' answers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a dummy classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Random Forest Classifier with Random Subspace\n",
        "random_subspace_classifier = RandomForestClassifier(n_estimators=10, max_features='sqrt', random_state=42)\n",
        "\n",
        "# Fit the Random Forest on the training data\n",
        "random_subspace_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = random_subspace_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaYuB9SVPsTa",
        "outputId": "1a6f9397-1b87-478d-ce79-a63f816e52f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 12.3.1** Modify the parameters of randomness, and compare corresponding models' performances. At which thresholds the model is benefited and harmed? Explain."
      ],
      "metadata": {
        "id": "QogNHfjiYKLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.4 Boosting"
      ],
      "metadata": {
        "id": "KfWWy_FaCrqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iterative unlike bagging where models are trained separately\n",
        "# new models are focused on instances handled incorrectly by earlier ones.\n",
        "# incorrect instances are given more weights, so next models are more biased towards them\n",
        "# weighting models' contributions by their performance, rather than giving equal weight to all models\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a dummy classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the AdaBoostClassifier\n",
        "adaboost_classifier = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Fit the AdaBoostClassifier on the training data\n",
        "adaboost_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBxCGYbAQ4__",
        "outputId": "5d100af6-6956-44b5-8044-25d8f47223ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 12.4.1** Rather than giving slightly more weights to misclassified instances, give them 100% of weights, totally ignoring correctly classified instances. What do you observe? Explain."
      ],
      "metadata": {
        "id": "eRsQbKtQXv1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.5 Additive Regression"
      ],
      "metadata": {
        "id": "ZGBnRCvqCt2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward stagewise additive modeling.\n",
        "# starts with an empty ensemble and incorporates new members sequentially.\n",
        "# At each stage the model that maximizes the predictive performance of the ensemble as a whole is added,\n",
        "# without altering those already in the ensemble.\n",
        "# next model should focus on those training instances on which the ensemble performs poorly.\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Create a dummy regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the GradientBoostingRegressor\n",
        "gradient_boosting_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Fit the GradientBoostingRegressor on the training data\n",
        "gradient_boosting_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gradient_boosting_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0e5Uf-PSOXT",
        "outputId": "ed7b1927-dbcf-4776-ad44-732d8e34c136"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 3052.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 12.5.1** Consider the misclassified instances at some stage. Compare the ensemble's performance among integrating different models into it."
      ],
      "metadata": {
        "id": "y5im7q0HXSQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.6 Interpretable Ensembles"
      ],
      "metadata": {
        "id": "99RFOYv8CxHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 12.6.1** Recall _model trees_ covered in previous notebooks, and think how boosting algorithm can be applied to build them."
      ],
      "metadata": {
        "id": "QSHV12p_VRmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12.7 Stacking"
      ],
      "metadata": {
        "id": "Ipd0lL0DC5RO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 12.7.1** Build a Naive Bayes learner, and an instance-based learning scheme and combine them to form a classifier by voting."
      ],
      "metadata": {
        "id": "dS1FaM2kV34l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The voting aggregation method you used does not conform to the _stacking_ methodology. The idea is to build a meta-model, i.e a model above many models, learning how to use them.\n",
        "\n",
        "**Task 12.7.2** Take the outputs of both Naive Bayes and instance-based model, and re-interpret them as inputs to a new model. Do simple statistical analysis and manually combine them by a rule of your choice. Alternatively build a meta-model."
      ],
      "metadata": {
        "id": "Yz2FA5DgWecp"
      }
    }
  ]
}